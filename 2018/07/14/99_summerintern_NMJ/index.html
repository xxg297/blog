<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/blog/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/blog/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="google5b248f7b86cbcee5.html" />














  
  
  <link href="/blog/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/blog/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/blog/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/blog/images/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/blog/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/blog/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="project,neural science,summer intern,connectomics,computational neural science,deep learning,computer vision,Jeff Lichtman," />





  <link rel="alternate" href="/blog/atom.xml" title="WonderLand" type="application/atom+xml" />






<meta name="description" content="It is my detailed progress of Neural Muscular Junction project during my summer intern in Jeff Lichtman Lab. With the generous help of Jeff, I complete NMJ tracing and segmentation work. The codes rel">
<meta name="keywords" content="project,neural science,summer intern,connectomics,computational neural science,deep learning,computer vision,Jeff Lichtman">
<meta property="og:type" content="article">
<meta property="og:title" content="NMJ Project">
<meta property="og:url" content="https://www.cmwonderland.com/blog/2018/07/14/99_summerintern_NMJ/index.html">
<meta property="og:site_name" content="WonderLand">
<meta property="og:description" content="It is my detailed progress of Neural Muscular Junction project during my summer intern in Jeff Lichtman Lab. With the generous help of Jeff, I complete NMJ tracing and segmentation work. The codes rel">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://i2.tiimg.com/640680/674ef868297b66d0.gif">
<meta property="og:image" content="http://i1.fuimg.com/640680/45e727fc31a9b0dc.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/8b18f43f830a2eb7.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/2a5303ed7c054769.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/96c7c89bc409d6ba.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/070eef826c0612ae.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/2b1b68b735d93e29.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/c9d99655bdaf9cac.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/06d9ead45f4caa9a.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/68a5d5858d2eca5f.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/14e81611ee917fa6.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/20dac5fc8da5ae1f.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/6ba0407959138c82.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/83ebcf388707f0a6.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/f618886ef775eb3c.png">
<meta property="og:image" content="http://i4.fuimg.com/640680/080f9880226ba6e5.png">
<meta property="og:image" content="http://i4.fuimg.com/640680/493a642ca564813f.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/96c7c89bc409d6ba.png">
<meta property="og:image" content="http://i4.fuimg.com/640680/743789aaa8f0fe9f.png">
<meta property="og:image" content="http://i4.fuimg.com/640680/8c9abba357fcb56c.png">
<meta property="og:image" content="http://i4.fuimg.com/640680/ad6c8f42a22e89f1.png">
<meta property="og:image" content="http://i4.fuimg.com/640680/db41e29e1b9c045a.png">
<meta property="og:image" content="http://i4.fuimg.com/640680/c8462edb973660e0.png">
<meta property="og:image" content="http://i4.fuimg.com/640680/81ada9efd84b2bb8.png">
<meta property="og:image" content="http://i4.fuimg.com/640680/87378f2e7ea1db4c.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/89d953939c17fce1.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/584c64fdaf11c64e.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/9d013476a19d3dd8.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/3c2d8c2fe9cab2e7.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/9e61c84bcda13810.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/688954a7b2f85b05.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/d7f84ed10dc6977f.png">
<meta property="og:image" content="http://i1.fuimg.com/640680/3e78d3e458b063b3.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/70f0977b0a1fdfa5.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/3b59f2842eaf0b18.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/713830f2720ff701.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/3ccf827be716fafb.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/e24603eb94d53f89.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/840d29a47250f731.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/1b3e88542e0b6375.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/659916b155960e05.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/b064a79497b7cf37.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/c5b7f35d11a3cc82.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/749f5a6947ca871b.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/0cc8c91faa68fc5c.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/845ff7279a00f5f0.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/7b9b3d2b77f9f277.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/f06f7b963b5b305c.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/59ecac8427b3a117.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/43d03374f4d2abd3.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/3e61fe8864bdff61.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/c8017110826d1fd5.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/1ecfd3d4046d2dda.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/a059b010a2dd1e39.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/ee09bea1b5c618d4.png">
<meta property="og:image" content="http://i2.tiimg.com/640680/5b0f66cfd920ac58.png">
<meta property="og:updated_time" content="2019-04-23T13:07:13.890Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="NMJ Project">
<meta name="twitter:description" content="It is my detailed progress of Neural Muscular Junction project during my summer intern in Jeff Lichtman Lab. With the generous help of Jeff, I complete NMJ tracing and segmentation work. The codes rel">
<meta name="twitter:image" content="http://i2.tiimg.com/640680/674ef868297b66d0.gif">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/blog/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://www.cmwonderland.com/blog/2018/07/14/99_summerintern_NMJ/"/>





  <title>NMJ Project | WonderLand</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?6656499bfc0e07b4e20ee4975eb85f31";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <a href="https://github.com/james20141606"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_white_ffffff.png" alt="Fork me on GitHub"></a>
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/blog/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">WonderLand</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Somnium & Somniator</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/blog/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/blog/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/blog/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/blog/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/blog/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.cmwonderland.com/blog/blog/2018/07/14/99_summerintern_NMJ/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="James Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/blog/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WonderLand">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">NMJ Project</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-14T20:35:19+08:00">
                2018-07-14
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/projects/" itemprop="url" rel="index">
                    <span itemprop="name">projects</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/blog/2018/07/14/99_summerintern_NMJ/" class="leancloud_visitors" data-flag-title="NMJ Project">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  6,171
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  39
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>It is my detailed progress of <strong>Neural Muscular Junction project</strong> during my summer intern in <a href="https://lichtmanlab.fas.harvard.edu/" target="_blank" rel="noopener">Jeff Lichtman Lab</a>. With the generous help of Jeff, I complete NMJ tracing and segmentation work.</p>
<p>The <strong>codes related</strong> are here:<br><a href="https://github.com/james20141606/Summer_Intern" target="_blank" rel="noopener">Main codes</a><br><a href="https://github.com/james20141606/NMJ_automatic_pipeline" target="_blank" rel="noopener">Automatic pipeline</a></p>
<p>For <strong>whole work summary</strong> please <a href="https://www.cmwonderland.com/blog/2018/09/12/100_summer_intern/">see here</a></p>


	<div class="row">
		<iframe src="https://drive.google.com/file/d/1XyEPj9r7p8VNk5nLlLIPNhZjuDHu2KTY/preview" style="width:100%; height:550px"></iframe>
	</div>



<p>Also I finished another synapse prediction and synaptic polarity prediction work during summer intern in <a href="https://vcg.seas.harvard.edu/people" target="_blank" rel="noopener">Hanspiter Lab</a></p>
<a id="more"></a>
<h1 id="weekly-report"><a href="#weekly-report" class="headerlink" title="weekly report"></a>weekly report</h1><h2 id="First"><a href="#First" class="headerlink" title="First"></a>First</h2>

	<div class="row">
		<iframe src="https://drive.google.com/file/d/1bmWX9M1aTgOw7YB9xrnUNynWfxuoa_Rz/preview" style="width:100%; height:550px"></iframe>
	</div>



<h2 id="Second"><a href="#Second" class="headerlink" title="Second"></a>Second</h2>

	<div class="row">
		<iframe src="https://drive.google.com/file/d/1OOaFajkLcwQBEf1XQuEIYqFAkl9psPrt/preview" style="width:100%; height:550px"></iframe>
	</div>



<h2 id="Third"><a href="#Third" class="headerlink" title="Third"></a>Third</h2>

	<div class="row">
		<iframe src="https://drive.google.com/file/d/1We4g3Ltd2gqzmICoLtKFKsL-2zP2BVeV/preview" style="width:100%; height:550px"></iframe>
	</div>



<h2 id="Fourth"><a href="#Fourth" class="headerlink" title="Fourth"></a>Fourth</h2>

	<div class="row">
		<iframe src="https://drive.google.com/file/d/19zhoBM6GP94a-bNj7bFIZdExC0x6yw_2/preview" style="width:100%; height:550px"></iframe>
	</div>



<hr>
<h1 id="First-two-weeks"><a href="#First-two-weeks" class="headerlink" title="First two weeks"></a>First two weeks</h1><p>Since the new data is still to be processed, I spent several days doing dense segmentation work both for study and future training. </p>
<p>I have done 25 sections dense segmentation in W12-W14 for 4 days(7.5-7.8), it includes dense segmentation of Axons, Schwann cell, and Schwann cell nucleus. </p>
<p>I have written codes to use python to visualize animation of the 25 segments <img src="http://i2.tiimg.com/640680/674ef868297b66d0.gif" alt="Markdown">. Since the computational  work use more python codes know, I also shared my animating code with others.<br><a href="https://github.com/james20141606/Summer_Intern/blob/master/NMJ/jupyter/plot_segment.ipynb" target="_blank" rel="noopener">plot segment script</a></p>
<p>Core codes to plot animation in python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">defdef  transform_rgbtransfor (img):</span><br><span class="line">    num = np.unique(img.reshape(<span class="number">-1</span>,<span class="number">3</span>),axis=<span class="number">0</span>).shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment">#print (num)</span></span><br><span class="line">    <span class="comment">#rgbarr = np.ndarray([num*3])</span></span><br><span class="line">    <span class="comment">#for i in range(num*3):</span></span><br><span class="line">      <span class="comment">#  rgbarr[i] = np.random.uniform(0,1)</span></span><br><span class="line">    <span class="comment">#rgbarr = rgbarr.reshape(-1,3)</span></span><br><span class="line">    image = np.zeros([img.shape[<span class="number">0</span>]*img.shape[<span class="number">1</span>],<span class="number">3</span>])</span><br><span class="line">    sumimg = np.sum(img.reshape(<span class="number">-1</span>,<span class="number">3</span>),axis=<span class="number">1</span>)</span><br><span class="line">    uniqueind = np.unique(img.reshape(<span class="number">-1</span>,<span class="number">3</span>),axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="number">0</span>,num<span class="number">-1</span>):</span><br><span class="line">        image[sumimg==<span class="number">3</span>*(uniqueind[i][<span class="number">0</span>]+<span class="number">1</span>)] = colorsgallery[i]</span><br><span class="line">    <span class="comment">#print (sumimg.shape)</span></span><br><span class="line">    image[sumimg==<span class="number">0</span>] = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> image.reshape(img.shape[<span class="number">0</span>],img.shape[<span class="number">1</span>],<span class="number">3</span>)</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.animation <span class="keyword">as</span> animation</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> rc</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">animations</span><span class="params">(opt=<span class="string">'show'</span>,type=<span class="string">'gif'</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    opt: show/save</span></span><br><span class="line"><span class="string">    type:gif/mp4</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    imagelist = [transform_rgb(imagedata[i][<span class="number">100</span>:<span class="number">900</span>,<span class="number">100</span>:<span class="number">900</span>]) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">26</span>)]</span><br><span class="line">    fig,ax=plt.subplots(<span class="number">1</span>,figsize=(<span class="number">16</span>,<span class="number">12</span>)) </span><br><span class="line">    im =ax.imshow(imagelist[<span class="number">0</span>])</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">updatefig</span><span class="params">(j)</span>:</span></span><br><span class="line">        im.set_array(imagelist[j])</span><br><span class="line">        <span class="keyword">return</span> [im]</span><br><span class="line">    anim = animation.FuncAnimation(fig, updatefig, frames=range(<span class="number">26</span>), </span><br><span class="line">                                  interval=<span class="number">100</span>, blit=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">if</span> opt==<span class="string">'show'</span>:</span><br><span class="line">        <span class="keyword">return</span> anim</span><br><span class="line">    <span class="keyword">elif</span> opt==<span class="string">'save'</span>:</span><br><span class="line">        <span class="keyword">if</span> type==<span class="string">'mp4'</span>:</span><br><span class="line">            Writer = animation.writers[<span class="string">'ffmpeg'</span>]</span><br><span class="line">            writer1 = Writer(fps=<span class="number">10</span>)</span><br><span class="line">            anim.save(<span class="string">'animation.'</span>+type, writer=writer1,dpi=<span class="number">1000</span>)</span><br><span class="line">        <span class="keyword">elif</span> type==<span class="string">'gif'</span>:</span><br><span class="line">            <span class="comment">#Writer = animation.writers['imagemagick']</span></span><br><span class="line">            anim.save(<span class="string">'animation.'</span>+type, writer=<span class="string">'imagemagick'</span>, fps=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>I also read several articles Marco and Yaron recommened, including previous NMJ work, some segmentation and Connectome processing pipeline papers.</p>
<h2 id="Future-work"><a href="#Future-work" class="headerlink" title="Future work"></a>Future work</h2><p>We also discuss a lot about the future plan of the project. Since it is more challenging than other tasks, it seems we are a little slow in progress. We have worked with Marco to find a way to label the ROI and use a script to extract coordinates of the bounding box. We have labeled one mask, later we will test Adi’s align results and generate more.</p>
<ul>
<li>Manually create ROI region for bundles and NMJ for alignment</li>
<li>Do segmentation and \textbf{statistical analysis} work on some NMJs(concerning our limited staying time, it seems there isn’t enough time to wait for all NMJs’ alignment and segmentation results to analyze)</li>
</ul>
<hr>
<h1 id="Week-3"><a href="#Week-3" class="headerlink" title="Week 3"></a>Week 3</h1><h2 id="mask-and-seeding"><a href="#mask-and-seeding" class="headerlink" title="mask and seeding"></a>mask and seeding</h2><p>This week we have worked out a plan on alignment and seeding.</p>
<p>We use a mind map to record tree’s nodes to visualize our progress. The mask was sent to Adi for aligning. The alignment results seem very good.</p>
<p>I have seeded three masks Adi sent back, <strong>for about 900 sections mainly in bundle area.</strong></p>
<p>I also wrote python script <a href="https://github.com/james20141606/Summer_Intern/blob/master/NMJ/jupyter/plot_segment.ipynb" target="_blank" rel="noopener">Summer_Intern/plot_segment.ipynb at master · james20141606/Summer_Intern · GitHub</a> for further analysis. Since I am proficient in using python for visualization, statistical analysis and machine learning, I wrote some python scripts to read seeding result, visualize them and plot them in 3D and animation. It will be better to have more statistical analysis when I collect more seeding data.<br><img src="http://i1.fuimg.com/640680/45e727fc31a9b0dc.png" alt="Markdown"></p>
<h2 id="discussion-on-mask"><a href="#discussion-on-mask" class="headerlink" title="discussion on mask"></a>discussion on mask</h2><p>When we put masks on ROI, we found many branches even in bundle area, two branches from one stem may encounter and form a closed loop. We are worried if it will be a problem when we merge all masks together. After discussing with Adi and Daniel, we understand that the spatial structure’s change isn’t a big problem.</p>
<hr>
<p>Week 4</p>
<h2 id="My-thought-about-how-the-whole-project"><a href="#My-thought-about-how-the-whole-project" class="headerlink" title="My thought about how the whole project"></a>My thought about how the whole project</h2><p>This week I continue to seed on Mask3, and then I do a lot of exploration on how to do the NMJ project automatically.</p>
<p>I have understood how big and challenging this project is, it requires so many manually labeling work than we can’t finish all the masking and seeding and segmentation and reconstruction work in two months. We know that it took KK and Marco several months to finish part of the bundle parts. But the remaining parts are more complex to seed, segment and it contains maybe 200 masks with approximately 50,000 sections. It is hard to estimate how long it will take to finish the whole project</p>
<p>However, as I am getting more familiar with this project, I am trying to build a more automatically pipeline for seeding, predicting membrane and segmentation. If it works, the project may move faster when we are here and after we leave:)</p>
<h3 id="Seeding-on-Mask3"><a href="#Seeding-on-Mask3" class="headerlink" title="Seeding on Mask3"></a>Seeding on Mask3</h3><p>I felt that seeding on mask3 is much more complex than previous bundle seeding, the axon travels very fast and I should look up and down to look for one axon, it takes much more time to trace the branch than the main bundle.</p>
<h2 id="Automatic-pipeline"><a href="#Automatic-pipeline" class="headerlink" title="Automatic pipeline"></a>Automatic pipeline</h2><p><strong>We would like to build up a more automatic pipeline before we leave and test the whole pipeline on several masks to see if they can be merged and reconstructed.</strong></p>
<p>We would like to build up the whole pipeline, prepare all the codes and model for prediction and processing and write down the protocol.</p>
<p>The complete pipeline should contain:<br><strong>Generating Masks —&gt; Seeding —&gt; Predict Membrane —&gt; Expand Seeds —&gt; Merge different Masks</strong></p>
<p>Previously we do seeding manually and then predict membrane, but the remaining masks have so many sections, I would like to do the seeding work more automatically too.</p>
<h3 id="Predict-Membrane"><a href="#Predict-Membrane" class="headerlink" title="Predict Membrane"></a>Predict Membrane</h3><p>The automatically prediction parts must include membrane prediction, because it is “easier” to predict since the raw image already have the membrane.</p>
<h3 id="Automatically-seeding"><a href="#Automatically-seeding" class="headerlink" title="Automatically seeding"></a>Automatically seeding</h3><p>The traditional way is to manually put seeds on each axon, but we have approximately 50,000 sections if all masks are generated, it is so time-consuming to manually put seeds. I will <strong>generate seeds by distance transformation from membrane</strong></p>
<p>Then the seeds must be indexed to track each seed is from which axon, so we will manually put seeds  per 100 sections, then do <strong>Hungarian matching.</strong></p>
<h3 id="segmentation"><a href="#segmentation" class="headerlink" title="segmentation"></a>segmentation</h3><p>Expand the seed to generate segments</p>
<h3 id="Merge-masks"><a href="#Merge-masks" class="headerlink" title="Merge masks"></a>Merge masks</h3><p>We are thinking about linear interpolation to merge anchor sections for loop problems. We will discuss it more with Daniel and Yaron after the segmentation</p>
<h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>The related codes are here:<br><a href="https://github.com/james20141606/membrane_prediction" target="_blank" rel="noopener">GitHub - james20141606/membrane_prediction: Use 3D U-net to predict membrane predition</a></p>
<h3 id="Predict-Membrane-1"><a href="#Predict-Membrane-1" class="headerlink" title="Predict Membrane"></a>Predict Membrane</h3><p>I will use a 3D U-net model to use contours extracted from dense segmentation sections. Use 50 sections for training, then predict more, proofread predicted sections to generate more training samples. <strong>The iterative training and predicting method will make the model more precise.</strong></p>
<p>The model’s weight is adaptive to the pixels ratio, I can do fine tune on the model iteratively. So the model will be more precise and requires fewer proofreading. Last week I do many augmentation works, it is also useful to generate more training images since I only have 50 sections for training now.</p>
<p><strong>How to fine tune:</strong><br>If we want better result, we can manually label several sections on each mask and retrain the model on each mask.</p>
<p>For membrane prediction, since we do not consider affinity, we can also consider 2D U-net, it contains much less parameters and easier to train.</p>
<h3 id="Automatically-seeding-1"><a href="#Automatically-seeding-1" class="headerlink" title="Automatically seeding"></a>Automatically seeding</h3><ul>
<li><strong>Distance transformation</strong> to generate seeds from membrane</li>
<li><strong>Hungarian matching</strong> to label each seeds for different axons. Manually label one section’ s seed and do Hungarian matching for the next 100 sections.</li>
</ul>
<h3 id="Watershed"><a href="#Watershed" class="headerlink" title="Watershed"></a>Watershed</h3><p>Use watershed to expand seeds and generate segments</p>
<h3 id="Useful-resources"><a href="#Useful-resources" class="headerlink" title="Useful resources"></a>Useful resources</h3><p><a href="https://github.com/tdedecko/hungarian-algorithm/blob/master/hungarian.py#L6" target="_blank" rel="noopener">hungarian-algorithm/hungarian.py at master · tdedecko/hungarian-algorithm · GitHub</a><br><a href="https://github.com/hrldcpr/hungarian" target="_blank" rel="noopener">GitHub - hrldcpr/hungarian: Hungarian / Munkres’ algorithm for the linear assignment problem, in Python</a></p>
<p><a href="https://github.com/microns-ariadne/pipeline_engine/tree/cf100202997d3c848a21de441e15deb9f975042d/ariadne_microns_pipeline/tasks" target="_blank" rel="noopener">pipeline_engine/ariadne_microns_pipeline/tasks at cf100202997d3c848a21de441e15deb9f975042d · microns-ariadne/pipeline_engine · GitHub</a></p>
<p>Other possible algorithm:</p>
<ul>
<li>seeding<br>Use EM data to predict seeds, train seeding prediction network, using affinity because the axon travels fast. Sebastian’s group has some work. But I think it is imprecise compared to membrane prediction—distance transformation algorithm</li>
</ul>
<blockquote>
<p>Convolutional Networks Can Learn to Generate Affinity<br>Graphs for Image Segmentation<br>Maximin affinity learning of image segmentation</p>
</blockquote>
<p><a href="https://github.com/jiwoon-ahn/psa" target="_blank" rel="noopener">GitHub - jiwoon-ahn/psa: Learning Pixel-level Semantic Affinity with Image-level Supervision for Weakly Supervised Semantic Segmentation, CVPR 2018</a></p>
<h2 id="Work-on-membrane-prediction"><a href="#Work-on-membrane-prediction" class="headerlink" title="Work on membrane prediction"></a>Work on membrane prediction</h2><h3 id="Prepare-ground-truth-training-set"><a href="#Prepare-ground-truth-training-set" class="headerlink" title="Prepare ground truth training set"></a>Prepare ground truth training set</h3><p>I have started on membrane prediction pipeline after discussion with zudi, yaron and others. I would like to use previously label siyan and I have done in first two weeks to save time. We have done dense segmentation on 51 sections, I wrote a python script</p>
<p>Codes: <a href="https://github.com/james20141606/Summer_Intern/blob/master/NMJ/jupyter/extract_membrane_gt.ipynb" target="_blank" rel="noopener">Summer_Intern/extract_membrane_gt.ipynb at master · james20141606/Summer_Intern · GitHub</a></p>
<p> to extract the needed EM image and contours of the membrane in the following steps:</p>
<ul>
<li>export segmentation and EM ROI from VAST</li>
<li>read in python, converting id array to RGB array for visualization</li>
</ul>
<p><img src="http://i2.tiimg.com/640680/8b18f43f830a2eb7.png" alt="Markdown"></p>
<ul>
<li>find bounding box of each segmentation and EM image</li>
</ul>
<figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def find_bounding(data):</span><br><span class="line">    xmin = <span class="built_in">np</span>.<span class="built_in">sort</span>(<span class="built_in">np</span>.where(data[:,:,<span class="number">0</span>]!=<span class="number">0</span>)[<span class="number">0</span>])[<span class="number">0</span>]</span><br><span class="line">    xmax = <span class="built_in">np</span>.<span class="built_in">sort</span>(<span class="built_in">np</span>.where(data[:,:,<span class="number">0</span>]!=<span class="number">0</span>)[<span class="number">0</span>])[-<span class="number">1</span>]</span><br><span class="line">    ymin = <span class="built_in">np</span>.<span class="built_in">sort</span>(<span class="built_in">np</span>.where(data[:,:,<span class="number">0</span>]!=<span class="number">0</span>)[<span class="number">1</span>])[<span class="number">0</span>]</span><br><span class="line">    ymax = <span class="built_in">np</span>.<span class="built_in">sort</span>(<span class="built_in">np</span>.where(data[:,:,<span class="number">0</span>]!=<span class="number">0</span>)[<span class="number">1</span>])[-<span class="number">1</span>]</span><br><span class="line">    <span class="built_in">return</span> xmin, xmax, ymin, ymax</span><br><span class="line"><span class="built_in">row</span> = <span class="number">26</span></span><br><span class="line">fig,ax=plt.subplots(<span class="built_in">row</span>,<span class="number">2</span>,figsize=(<span class="number">16</span>,<span class="number">6</span>*<span class="built_in">row</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">row</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">        xmin, xmax, ymin, ymax = find_bounding(segdata[i*<span class="number">2</span>+j])</span><br><span class="line">        ax[i,j].imshow(transform_rgb(segdata[i*<span class="number">2</span>+j][(xmin-<span class="number">10</span>): (xmax+<span class="number">10</span>), (ymin-<span class="number">10</span>): (ymax+<span class="number">10</span>)]))</span><br></pre></td></tr></table></figure>
<ul>
<li>remove Schwann cell to concentrate on axons</li>
</ul>
<p>First it has some problems</p>
<p><img src="http://i2.tiimg.com/640680/2a5303ed7c054769.png" alt="Markdown"></p>
<p>Then I separately plot and find the black wrong region is 25 and 38</p>
<p>After correction:</p>
<p><img src="http://i2.tiimg.com/640680/96c7c89bc409d6ba.png" alt="Markdown"></p>
<ul>
<li>convert the segment array to binary mask</li>
</ul>
<p><img src="http://i2.tiimg.com/640680/070eef826c0612ae.png" alt="Markdown"></p>
<ul>
<li>Make sure each mask and EM data are in same bounding box</li>
</ul>
<p><img src="http://i2.tiimg.com/640680/2b1b68b735d93e29.png" alt="Markdown"></p>
<ul>
<li>Generate contours as training set label<br>opencv’s findcontour function is not suitable for same grayscale image, so I use erode and dilation<figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">open = cv2.erode(grayimg, None, iterations = 4)</span><br><span class="line">open1 = cv2.dilate(open, None, iterations = 3)</span><br><span class="line">imshow(open1-open)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><img src="http://i2.tiimg.com/640680/c9d99655bdaf9cac.png" alt="Markdown"></p>
<ul>
<li>Padding for same image size<br>Do reflection padding on each image to generate images with same size.</li>
</ul>
<p><img src="http://i2.tiimg.com/640680/06d9ead45f4caa9a.png" alt="Markdown"></p>
<p>The margin is the reflection of the original image<br>Store in HDF5</p>
<ul>
<li>Save image and label as HDF5<br>EM data as training set’s image and contour as label</li>
</ul>
<p>Save as uint8   (51, 530, 835)</p>
<h3 id="Train-membrane-prediction-model"><a href="#Train-membrane-prediction-model" class="headerlink" title="Train membrane prediction model"></a>Train membrane prediction model</h3><p>Input image and label are the 51 bundle sections.</p>
<p><strong>Train model args:</strong><br>Run on two machines: one with one gpu and another with 4 gpus</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span> python3 -u bin/train.py -t data/train_set/ -dn em_51 -ln mask_51 -o outputs/nmj0729mixloss -lr <span class="number">0.001</span> --volume-total <span class="number">40000</span> --volume-save <span class="number">2000</span> -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">1</span> -c <span class="number">12</span> -b <span class="number">4</span> -lt <span class="number">4</span> -ac <span class="number">2</span></span><br><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span> python3 -u bin/train.py -t data/train_set/ -dn em_51 -ln mask_51 -o outputs/nmj0729mixloss -lr <span class="number">0.001</span> --volume-total <span class="number">40000</span> --volume-save <span class="number">2000</span> -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">1</span> -c <span class="number">6</span> -b <span class="number">1</span> -lt <span class="number">4</span> -ac <span class="number">2</span></span><br><span class="line"></span><br><span class="line">#-lt <span class="number">4</span> focal and dice loss</span><br></pre></td></tr></table></figure>
<p>References:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span>,<span class="number">1</span> python3 -u bin/synapse_pytorch/train<span class="selector-class">.py</span> -t data/cremi/ -dn images/im_A_v2_200.h5@images/im_B_v2_200.h5@images/im_C_v2_200<span class="selector-class">.h5</span> -ln gt-syn/syn_A_v2_200.h5@gt-syn/syn_B_v2_200.h5@gt-syn/syn_C_v2_200<span class="selector-class">.h5</span> -o outputs/cremi0719mixloss -lr <span class="number">0.001</span> --volume-total <span class="number">400000</span> --volume-save <span class="number">20000</span> -mi <span class="number">24</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">2</span> -c <span class="number">6</span> -<span class="selector-tag">b</span> <span class="number">2</span> -l mix</span><br><span class="line"><span class="selector-id">#b</span>:<span class="number">6</span>  try to keep gpu and batch size same</span><br></pre></td></tr></table></figure>
<p>The hp003 memory is small, also train on hpc</p>
<h4 id="Check-loss"><a href="#Check-loss" class="headerlink" title="Check loss"></a>Check loss</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard <span class="attribute">--logdir</span>=outputs/nmj0729mixloss</span><br></pre></td></tr></table></figure>
<p>Monitor loss and test on new EM image. If is good, train it longer with more GPUs</p>
<p>Train loss( DICE + Focal loss)</p>
<p><img src="http://i2.tiimg.com/640680/68a5d5858d2eca5f.png" alt="Markdown"></p>
<p>Focal loss</p>
<p><img src="http://i2.tiimg.com/640680/14e81611ee917fa6.png" alt="Markdown"></p>
<p>Dice loss</p>
<p><img src="http://i2.tiimg.com/640680/20dac5fc8da5ae1f.png" alt="Markdown"></p>
<p>It seems that the combined loss and both focal and dice loss decrease well.</p>
<h4 id="real-time-monitoring-predicted-result"><a href="#real-time-monitoring-predicted-result" class="headerlink" title="real time monitoring predicted result"></a>real time monitoring predicted result</h4><p>Use TensorboardX to monitor predicted results:</p>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">if i % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="comment">#draw image every 20 batches</span></span><br><span class="line">            writer.add_image(<span class="string">'EM image '</span>+str(i),</span><br><span class="line">                             torchvision.utils.make_grid(<span class="keyword">volume</span><span class="bash">), i)</span></span><br><span class="line"><span class="bash">            writer.add_image(<span class="string">'GT image '</span>+str(i), torchvision.utils.make_grid(label), i)</span></span><br><span class="line"><span class="bash">            writer.add_image(<span class="string">'predict image '</span>+str(i), torchvision.utils.make_grid(output), i)</span></span><br></pre></td></tr></table></figure>
<p>This will allow me to see the improvement of model’s performance more clearly.</p>
<p>EM in 3680th batches</p>
<p><img src="http://i2.tiimg.com/640680/6ba0407959138c82.png" alt="Markdown"></p>
<p>Ground truth in 3680th batches</p>
<p><img src="http://i2.tiimg.com/640680/83ebcf388707f0a6.png" alt="Markdown"></p>
<p>Predicted in 3680th batches</p>
<p><img src="http://i2.tiimg.com/640680/f618886ef775eb3c.png" alt="Markdown"></p>
<p>Then I will predict new EM image which is preprocessed by the previous steps. Then do proofreading on the predicted membrane. Then do distance transformation to generate seeds.</p>
<hr>
<h1 id="Week-5-amp-6"><a href="#Week-5-amp-6" class="headerlink" title="Week 5 &amp; 6"></a>Week 5 &amp; 6</h1><h1 id="predict-on-EM"><a href="#predict-on-EM" class="headerlink" title="predict on EM"></a>predict on EM</h1><ul>
<li>prepare predict data</li>
<li>export mask1 em</li>
<li>Mip level 3, it is really important to keep the resolution same( realize it after several failures)</li>
<li>set to window</li>
<li>bad slices record, replace by the previous layer</li>
</ul>
<p><strong>Record export coordinates</strong></p>
<pre><code>- 0-80: 10809-20685  5448-11102
- 81-144 9200-19076   4649-10303
- 145-182 8193-18069  4104-9758
- 183-219  7337-17213  3792-9446
- 220-265 6496-16372  3584-9238
- 266-292   55509-15385    3241-8895
- 293-300 4590-14466   2914-8568
</code></pre><p><strong>Test commands</strong><br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span> python3 -u bin/test.py -t data/mask1/ -dn mask1_em.h5 -o outputs/mask1output -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">1</span> -m outputs/nmj0729mixloss/volume_40000.pth -c <span class="number">12</span> -b <span class="number">4</span> -ac <span class="number">2</span></span><br></pre></td></tr></table></figure></p>
<ul>
<li>see results:<br><a href="http://127.0.0.1:8889/notebooks/projects/membrane/jupyter/visualize_prediction_result.ipynb" target="_blank" rel="noopener">http://127.0.0.1:8889/notebooks/projects/membrane/jupyter/visualize_prediction_result.ipynb</a></li>
</ul>
<p>Not good, check very bad slices and deflicker<br>Try to train for a longer time</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span> python3 -u bin/train.py -t data/train_set/ -dn em_51 -ln mask_51 -o outputs/nmj0731retrain -lr <span class="number">0.0005</span> --volume-total <span class="number">400000</span> --volume-save <span class="number">4000</span> -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">3</span> -c <span class="number">8</span> -b <span class="number">3</span> -lt <span class="number">4</span> -ac <span class="number">2</span></span><br><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span> python3 -u bin/train.py -t data/train_set/ -dn em_51 -ln mask_51 -o outputs/nmj0729mixloss -lr <span class="number">0.001</span> --volume-total <span class="number">40000</span> --volume-save <span class="number">2000</span> -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">1</span> -c <span class="number">6</span> -b <span class="number">1</span> -lt <span class="number">4</span> -ac <span class="number">2</span></span><br><span class="line"></span><br><span class="line">#-lt <span class="number">4</span> focal and dice loss</span><br></pre></td></tr></table></figure>
<p>It seems the results of membrane prediction isn’t very good. Needs many post process work. For example, one way is to use a deep learning model like U-net to predict affinity and close the membrane.</p>
<p><img src="http://i4.fuimg.com/640680/080f9880226ba6e5.png" alt="Markdown"></p>
<h4 id="preprocess"><a href="#preprocess" class="headerlink" title="preprocess"></a>preprocess</h4><p><strong>Deflickering work</strong><br>I use deflickering codes to smooth the contrast.</p>
<ul>
<li>[ ] <a href="https://github.com/donglaiw/EM-preprocess/blob/master/script/T_deflicker.py" target="_blank" rel="noopener">EM-preprocess/T_deflicker.py at master · donglaiw/EM-preprocess · GitHub</a><br>20 sec to process a volume with data size 100x1024x1024 using online version of deflickering</li>
</ul>
<p><img src="http://i4.fuimg.com/640680/493a642ca564813f.png" alt="Markdown"></p>
<h4 id="try-to-predict-directly-on-masks-Not-the-membrane"><a href="#try-to-predict-directly-on-masks-Not-the-membrane" class="headerlink" title="try to predict directly on masks. Not the membrane"></a>try to predict directly on masks. Not the membrane</h4><p>It may have some advantages: do not need to be precise, we can perform distance transformation on the predicted masks to get the seed. And it will be better to track and automatically assign labels using masks instead of seeds.</p>
<p><img src="http://i2.tiimg.com/640680/96c7c89bc409d6ba.png" alt="Markdown"></p>
<p>Comparison of mask and membrane:<br><img src="http://i4.fuimg.com/640680/743789aaa8f0fe9f.png" alt="Markdown"></p>
<h3 id="7-31-retrain-on-deflicker-data"><a href="#7-31-retrain-on-deflicker-data" class="headerlink" title="7.31 retrain on deflicker data"></a>7.31 retrain on deflicker data</h3><p>volume_168000.pth<br>data/mask1/deflicker_em.h5</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span> python3 -u bin/test.py -t data/mask1/ -dn deflicker_em.h5 -o outputs/mask1output8<span class="number">.01</span>deflicker -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">3</span> -m outputs/nmj0801retrain/volume_156000.pth -c <span class="number">3</span> -b <span class="number">3</span> -ac <span class="number">2</span></span><br></pre></td></tr></table></figure>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span> python3 -u bin/train.py -t data/train_set/ -dn em_51 -ln mask_51 -o outputs/nmj0731debug -lr <span class="number">0.0005</span> --volume-total <span class="number">4000</span> --volume-save <span class="number">1000</span> -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">1</span> -c <span class="number">3</span> -b <span class="number">1</span> -lt <span class="number">4</span> -ac <span class="number">2</span></span><br><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span> python3 -u bin/test.py -t data/mask1/ -dn deflicker_em.h5 -o outputs/mask1outputdebug -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">1</span> -m outputs/nmj0731retrain/volume_4002.pth -c <span class="number">6</span> -b <span class="number">1</span> -ac <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>Model structure has problems<br>Retrain on previous model</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span> python3 -u bin/train.py -t data/train_set/ -dn em_51 -ln mask_51 -o outputs/nmj0801retrain -lr <span class="number">0.0001</span> --volume-total <span class="number">400000</span> --volume-save <span class="number">4000</span> -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">3</span> -c <span class="number">8</span> -b <span class="number">3</span> -lt <span class="number">4</span> -ac <span class="number">2</span> -ft True -pm outputs/nmj0731retrain/volume_200001.pth</span><br></pre></td></tr></table></figure>
<p>Train on segment data</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span> python3 -u bin/train.py -t data/train_set/ -dn em_51 -ln mask_51_ -o outputs/nmj0801segment -lr <span class="number">0.001</span> --volume-total <span class="number">400000</span> --volume-save <span class="number">4000</span> -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">3</span> -c <span class="number">8</span> -b <span class="number">3</span> -lt <span class="number">4</span> -ac <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>It is weird that after some training, the test results have nothing</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">5</span> python3 -u bin/train.py -t data/train_set/ -dn em_51 -ln mask_51 -o outputs/nmj0801after -lr <span class="number">0.001</span> --volume-total <span class="number">400000</span> --volume-save <span class="number">4000</span> -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">1</span> -c <span class="number">8</span> -b <span class="number">1</span> -lt <span class="number">4</span> -ac <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>Add image augmentation, adjust intensity augmentation. Decrease contrast and brightness ratio, it will severely influence converge.</p>
<h4 id="NMJ-manually-labeling-work"><a href="#NMJ-manually-labeling-work" class="headerlink" title="NMJ manually labeling work"></a>NMJ manually labeling work</h4><p>First I start randomly from a terminal or axon. Then Jeff recommended it is better to start from axons. I use the latter method to manually label <strong>two NMJs</strong> for using one week.</p>
<p>At first it seems very hard to track and label NMJs. The boundary is unclear and I have little experience on labeling NMJs, it is a lot harder to label NMJs than labeling on the main bundle.</p>
<p>I will try to label maybe more <strong>5 NMJs</strong> to collect enough data to test the linear hypothesis: <strong>the correlation of axon caliber and terminal area.</strong> If we can prove that, we can save a lot of time: we can just dense segment on axons and calculate the corresponding terminal area. And we only need tracing on the terminals.</p>
<p>I will try to reslice the labeled NMJs to calculate the axon caliber. It seems VTK is a good tool to do reslice on any arbitrary orientation reslice.</p>
<h4 id="Add-more-data-for-automatic-pipeline"><a href="#Add-more-data-for-automatic-pipeline" class="headerlink" title="Add more data for automatic pipeline"></a>Add more data for automatic pipeline</h4><ul>
<li>[ ] 代码 <a href="http://140.247.107.75:8889/notebooks/projects/membrane/jupyter/extract_membrane_from_marco.ipynb" target="_blank" rel="noopener">Jupyter Notebook</a><br>I processed marco’s data for training. It is really precious, we easily increase our data from 51 images to 1440 images. It is really helpful if we have more.</li>
</ul>
<figure class="highlight delphi"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> h5py.<span class="keyword">File</span>(<span class="string">'data/train_set/marco_1435_mask'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.create_dataset(<span class="string">'main'</span>,data= paddedmask,dtype =uint8)</span><br><span class="line"><span class="keyword">with</span> h5py.<span class="keyword">File</span>(<span class="string">'data/train_set/marco_1435_membrane'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.create_dataset(<span class="string">'main'</span>,data= paddedmaskmem,dtype =uint8)</span><br></pre></td></tr></table></figure>
<p><img src="http://i4.fuimg.com/640680/8c9abba357fcb56c.png" alt="Markdown"></p>
<p><img src="http://i4.fuimg.com/640680/ad6c8f42a22e89f1.png" alt="Markdown"></p>
<p><img src="http://i4.fuimg.com/640680/db41e29e1b9c045a.png" alt="Markdown"></p>
<h3 id="updated-pipeline"><a href="#updated-pipeline" class="headerlink" title="updated pipeline"></a>updated pipeline</h3><ul>
<li>segmentation first, doesn’t need to be very precise. </li>
<li>Then do distance transform and find the seed. It is easy to find connected component and then do distance transform to get seeds.</li>
<li><p>Test distance transform and hungarian matching on marco’s data.</p>
</li>
<li><p>[ ]  代码<a href="http://140.247.107.75:8889/notebooks/projects/membrane/jupyter/extract_membrane_from_marco.ipynb" target="_blank" rel="noopener">Jupyter Notebook</a></p>
</li>
</ul>
<p>8.3  training</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span> python3 -u bin/train.py -t data/train_set/marco/ -dn marco_1435_em -ln marco_1435_mask -o outputs/nmj0804segmentmarcodata -lr <span class="number">0.001</span> --volume-total <span class="number">400000</span> --volume-save <span class="number">4000</span> -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">1</span> -c <span class="number">4</span> -b <span class="number">1</span> -lt <span class="number">4</span> -ac <span class="number">2</span></span><br><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span> python3 -u bin/train.py -t data/train_set/marco/ -dn marco_1435_em -ln marco_1435_mask -o outputs/nmj0805segmentmarcodataretrain -lr <span class="number">0.001</span> --volume-total <span class="number">400000</span> --volume-save <span class="number">4000</span> -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">1</span> -c <span class="number">4</span> -b <span class="number">1</span> -lt <span class="number">4</span> -ac <span class="number">2</span> -ft True -pm outputs/nmj0805segmentmarcodataretrain/volume_4000.pth</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span> python3 -u bin/test.py -t data/mask1/ -dn mask1_em.h5 -o outputs/mask1output8<span class="number">.04</span> -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">1</span> -m outputs/nmj0804segmentmarcodata/volume_108000.pth -c <span class="number">2</span> -b <span class="number">1</span> -ac <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h4 id="resolution-matters"><a href="#resolution-matters" class="headerlink" title="resolution matters!"></a>resolution matters!</h4><p>It seems that the resolution influence the prediction. We should keep the training and test data in the same resolution!</p>
<p>Record test data’s export coordinates:</p>
<pre><code>- 0-80: 10809-20685  5448-11102
- 81-144 9200-19076   4649-10303
- 145-182 8193-18069  4104-9758
- 183-219  7337-17213  3792-9446
- 220-265 6496-16372  3584-9238
- 266-292   55509-15385    3241-8895
- 293-300 4590-14466   2914-8568
</code></pre><p>Try <strong>mip4</strong> result</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span> python3 -u bin/test.py -t data/mask1/ -dn mask1_em_mip4.h5 -o outputs/mask1output8<span class="number">.05</span>mip4 -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">1</span> -m outputs/nmj0804segmentmarcodata/volume_108000.pth -c <span class="number">2</span> -b <span class="number">1</span> -ac <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p><img src="http://i4.fuimg.com/640680/c8462edb973660e0.png" alt="Markdown"></p>
<p>Result still not good: some axons are not predicted.<br>Maybe the noise has a big influence. And the proposed region isn’t enough.  Maybe DICE loss function influence the False positive region.</p>
<p>Try only BCE</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span> python3 -u bin/train.py -t data/train_set/marco/ -dn marco_1435_em -ln marco_1435_mask -o outputs/nmj0805segmentmarcodataBCE -lr <span class="number">0.001</span> --volume-total <span class="number">400000</span> --volume-save <span class="number">4000</span> -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">1</span> -c <span class="number">4</span> -b <span class="number">1</span> -lt <span class="number">1</span> -ac <span class="number">2</span></span><br><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span> python3 -u bin/test.py -t data/mask1/ -dn mask1_em_mip4.h5 -o outputs/mask1output8<span class="number">.07</span>mip4 -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">1</span> -m outputs/nmj0805segmentmarcodataBCE/volume_160000.pth -c <span class="number">2</span> -b <span class="number">1</span> -ac <span class="number">2</span></span><br><span class="line"></span><br><span class="line">#<span class="number">8.7</span> retrain</span><br><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span> python3 -u bin/train.py -t data/train_set/marco/ -dn marco_1435_em -ln marco_1435_mask -o outputs/nmj0807segmentmarcodataretrain -lr <span class="number">0.001</span> --volume-total <span class="number">400000</span> --volume-save <span class="number">4000</span> -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">1</span> -c <span class="number">4</span> -b <span class="number">1</span> -lt <span class="number">4</span> -ac <span class="number">2</span> -ft True -pm outputs/nmj0805segmentmarcodataBCE/volume_160000.pth</span><br></pre></td></tr></table></figure>
<p>Results still not good<br><img src="http://i4.fuimg.com/640680/81ada9efd84b2bb8.png" alt="Markdown"></p>
<h4 id="8-11-new-try"><a href="#8-11-new-try" class="headerlink" title="8.11 new try"></a>8.11 new try</h4><p>Previous computing resource isn’t enough. Only have one gpu on the machine. I use RC cluster to do the computing. It has many gpus to use. Setting some environment and softwares to computing.</p>
<figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">srun <span class="params">--pty</span> -p cox -t 7-00<span class="function">:00</span> <span class="params">--mem</span> 100000 -n 8 <span class="params">--gres=gpu</span><span class="function">:4</span> <span class="string">/bin/bash</span></span><br><span class="line">srun <span class="params">--pty</span> -p cox -t 7-00<span class="function">:00</span> <span class="params">--mem</span> 200000 -n 2 <span class="params">--gres=gpu</span><span class="function">:1</span> <span class="string">/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#control D exit</span></span><br><span class="line"><span class="comment">#squeue/sacct check job</span></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> add all other SBATCH directives here...</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">SBATCH -p cox</span></span><br><span class="line"><span class="meta">#</span><span class="bash">SBATCH --gres=gpu:4</span></span><br><span class="line"><span class="meta">#</span><span class="bash">SBATCH --constraint=titanx</span></span><br><span class="line"><span class="meta">#</span><span class="bash">SBATCH -n 8 <span class="comment"># Number of cores</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">SBATCH -N 1 <span class="comment"># Ensure that all cores are on one machine</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">SBATCH --mem=100000</span></span><br><span class="line"><span class="meta">#</span><span class="bash">SBATCH -t 5-00:00:00</span></span><br><span class="line"><span class="meta">#</span><span class="bash">SBATCH -o logs/train_%j.log</span></span><br><span class="line"></span><br><span class="line">module load cuda</span><br><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=0,1,2,3 python3 -u bin/train.py -t data/train_set/marco/ -dn marco_1435_em -ln marco_1435_mask -o outputs/nmj08011segmentmarcodataretrain -lr 0.001 --volume-total 400000 --volume-save 4000 -mi 4,256,256 -g 4 -c 8 -b 4 -lt 4 -ac 2 -ft True -pm outputs/nmj0811membranemarcodata/volume_12000.pth</span><br><span class="line"><span class="meta">#</span><span class="bash"> end of program</span></span><br><span class="line">exit 0;</span><br></pre></td></tr></table></figure>
<p>Working dir on rc<br>/n/coxfs01/xupeng/projects/membrane<br>Scp -r hp003 to rc<br>Install anaconda2 and 3<br>Install pytorch(0.4.0)  keras and tensorflow<br>tensorboardX 1.2 torchvision0.2</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> keras</span><br><span class="line">pip <span class="keyword">install</span> tensorflow-gpu</span><br><span class="line">conda <span class="keyword">install</span> pytorch torchvision -c pytorch</span><br></pre></td></tr></table></figure>
<p>train with 4 gpus<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span> python3 -u bin/train.py -t data/train_set/marco/ -dn marco_1435_em -ln marco_1435_mask -o outputs/nmj0811membranemarcodata -lr <span class="number">0.001</span> --volume-total <span class="number">400000</span> --volume-save <span class="number">4000</span> -mi <span class="number">12</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">4</span> -c <span class="number">8</span> -b <span class="number">4</span> -lt <span class="number">4</span> -ac <span class="number">2</span></span><br><span class="line"></span><br><span class="line">#retrain</span><br><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span> python3 -u bin/train.py -t data/train_set/marco/ -dn marco_1435_em -ln marco_1435_mask -o outputs/nmj08012segmentmarcodataretrain -lr <span class="number">0.001</span> --volume-total <span class="number">400000</span> --volume-save <span class="number">4000</span> -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">4</span> -c <span class="number">8</span> -b <span class="number">4</span> -lt <span class="number">4</span> -ac <span class="number">2</span> -ft True -pm outputs/nmj0811membranemarcodata/volume_12000.pth</span><br></pre></td></tr></table></figure></p>
<p><strong>Thoughts about the not perfect results:</strong></p>
<ul>
<li>Maybe 3D U-net model is too large to train, intensity has influence. </li>
<li>The pattern difference is large, we may use the similar one with larger weights. </li>
<li>We may need some more design to predict many separate regions and consider the continuity. Consider higher resolution.</li>
</ul>
<h4 id="2D-D-Linknet"><a href="#2D-D-Linknet" class="headerlink" title="2D D-Linknet"></a>2D D-Linknet</h4><p>I started to build a new deep learning model. I use 2D U-net instead of 3D to train is easier. It is different with U-net, but also effective in predicting segments.</p>
<p><img src="http://i4.fuimg.com/640680/87378f2e7ea1db4c.png" alt="Markdown"></p>
<p>D-LinkNet uses Linknet with pretrained encoder as its backbone and has additional dilated convolution layers in the center part. Linknet is an efficient semantic segmentation<br>neural network which takes the advantages of skip connections, residual blocks and encoder-decoder architecture. The original Linknet uses ResNet18 as its encoder, which is a pretty light but outperforming network. Linknet has shown high precision on several benchmarks, and it runs pretty fast.</p>
<p>I also use dilation CNN, Dilated convolution is a useful kernel to adjust receptive<br>fields of feature points without decreasing the resolution of feature maps. It was widely used recently.</p>
<p>I set the image input shape as 1024*1024, so I reprocess Marco data, export Mip level 0. Change the export ROI for better ROI and more precise resolution. Change the channels for resnet.</p>
<p>I will keep on modifying the model and build up the whole training pipeline including several efficient data augmentation methods. And then do test on the mask data.</p>
<hr>
<h1 id="Last-three-weeks"><a href="#Last-three-weeks" class="headerlink" title="Last three weeks"></a>Last three weeks</h1><p>Week 7,8,9 (10)</p>
<ul>
<li>mip 0， train on old model<br>Process the mip 0 data<br><a href="http://140.247.107.75:10000/notebooks/projects/membrane/jupyter/extract_membrane_from_marco_mip0.ipynb" target="_blank" rel="noopener">Jupyter Notebook</a></li>
<li>mip 0, multi task</li>
<li>mip 0, 2D D-linknet<br>This is three and one channel, for 2D Linknet</li>
</ul>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">data</span>/train_set/marco/maskfor2Dlinknet.h5</span></span><br><span class="line"><span class="class"><span class="keyword">data</span>/train_set/marco/emfor2Dlinknet.h5</span></span><br></pre></td></tr></table></figure>
<ul>
<li>prepare dataloader 512 512, random slice to get data, augmentation, weight, test dense segment</li>
<li>change channel to 3 and 1 after dataloader’s original process !  </li>
</ul>
<p><code>np.stack((imgs,)*3, 1)</code></p>
<p>Solve dataloader problem<br>Test to use synapse data loader, it has random slice, but we need  dimension right and batch more than one</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span> python3 -u bin/train.py -t data/train_set/marco/ -dn marco_1435_em -ln marco_1435_mask -o outputs/testdimension -lr <span class="number">0.001</span> --volume-total <span class="number">400000</span> --volume-save <span class="number">4000</span> -mi <span class="number">2</span>,<span class="number">512</span>,<span class="number">512</span> -g <span class="number">1</span> -c <span class="number">1</span> -b <span class="number">1</span> -lt <span class="number">4</span> -ac <span class="number">2</span></span><br></pre></td></tr></table></figure>
<ul>
<li>512*512</li>
<li>normalization</li>
<li>augmentation</li>
<li>test dense</li>
</ul>
<p>Train use mip0<br>1434,1112, 1734<br>This is one channel, for 3D unet, too large to train, use half? Last 800</p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">data</span>/train_set/marco/coloredmaskmip0whole.h5</span></span><br><span class="line"><span class="class"><span class="keyword">data</span>/train_set/marco/emmip0whole.h5</span></span><br></pre></td></tr></table></figure>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span> python3 -u bin/train.py -t data/train_set/marco/ -dn emmip0whole_half.h5 -ln coloredmaskmip0whole_half.h5 -o outputs/nmj0813marcodatamip0 -lr <span class="number">0.001</span> --volume-total <span class="number">400000</span> --volume-save <span class="number">4000</span> -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">4</span> -c <span class="number">4</span> -b <span class="number">4</span> -lt <span class="number">4</span> -ac <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>On hp003 not work, memory error maybe should cut to quarter</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span> python3 -u bin/train.py -t data/train_set/marco/ -dn emmip0whole_quarter.h5 -ln coloredmaskmip0whole_quarter.h5 -o outputs/nmj0814marcodatamip0 -lr <span class="number">0.001</span> --volume-total <span class="number">400000</span> --volume-save <span class="number">1000</span> -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">1</span> -c <span class="number">4</span> -b <span class="number">1</span> -lt <span class="number">4</span> -ac <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>Locally smoothed networks<br>Smooth backgroud</p>
<p>talk about NMJ<br>Daniel and Jeff<br>Axon caliber:</p>
<ul>
<li>calculate volume and distance</li>
<li>or reslice it and find the minimum diameter!<br>Terminal:<br>Reslice it and paint the terminal!</li>
</ul>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#<span class="number">8.14</span>  test <span class="number">4</span> gpus result</span><br><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span> python3 -u bin/test.py -t data/mask1/ -dn mask1_em_mip4.h5 -o outputs/nmj08014segmentmarcodataretrainmip4 -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">4</span> -m outputs/nmj08012segmentmarcodataretrain/volume_400000.pth -c <span class="number">4</span> -b <span class="number">4</span> -ac <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>not good enough, almost sure it is about resolution<br><img src="http://i2.tiimg.com/640680/89d953939c17fce1.png" alt="Markdown"></p>
<p><strong>sbatch_mip0_3D.sh</strong></p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span> python3 -u bin/train.py -t data/train_set/marco/ -dn emmip0whole_half.h5 -ln coloredmaskmip0whole_half.h5 -o outputs/nmj0814marcodatamip0_half -lr <span class="number">0.001</span> --volume-total <span class="number">400000</span> --volume-save <span class="number">1000</span> -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">4</span> -c <span class="number">4</span> -b <span class="number">4</span> -lt <span class="number">4</span> -ac <span class="number">2</span></span><br></pre></td></tr></table></figure>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span> python3 -u bin/train.py -t data/train_set/marco/ -dn emmip0whole.h5 -ln coloredmaskmip0whole.h5 -o outputs/nmj0814marcodatamip0 -lr <span class="number">0.001</span> --volume-total <span class="number">400000</span> --volume-save <span class="number">1000</span> -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">4</span> -c <span class="number">4</span> -b <span class="number">4</span> -lt <span class="number">4</span> -ac <span class="number">2</span></span><br></pre></td></tr></table></figure>
<ul>
<li>outputs/nmj0814marcodatamip0</li>
<li>outputs/nmj0814marcodatamip0_half</li>
</ul>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span> python3 -u bin/test.py -t data/mask1/ -dn mask1_em_mip0_0_50.h5 -o outputs/nmj0815marcodatamip0_half -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">4</span> -m outputs/nmj0814marcodatamip0_half/volume_86000.pth -c <span class="number">4</span> -b <span class="number">4</span> -ac <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>logs/3D_mip0_hald_test.err</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span> python3 -u bin/test.py -t data/mask1/ -dn mask1_em_mip0_0_50.h5 -o outputs/nmj0815marcodatamip0 -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">1</span> -m outputs/nmj0814marcodatamip0/volume_99000.pth -c <span class="number">1</span> -b <span class="number">1</span> -ac <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>3D_mip0_whole_test.err</p>
<p>Conclusion:</p>
<p><strong>we should use 2D since the shift is too big to infer 3D information</strong></p>
<p>Finally the segmentation part of automatic tracing pipeline works:</p>
<h3 id="Automatic-Tracing-in-Bundle"><a href="#Automatic-Tracing-in-Bundle" class="headerlink" title="Automatic Tracing in Bundle"></a>Automatic Tracing in Bundle</h3><p><img src="http://i2.tiimg.com/640680/584c64fdaf11c64e.png" alt="Markdown"></p>
<p>This work is inspired from yaron and marco’s great work on automatically prediction membrane on bundle. And we are thinking, if we only care about tracing, maybe we can automatically trace the axon with little manual label. Since the bundle data is sparse and the shift of the z section is big, we may use a simpler yet more robust way to automaticaly trace.<br>So at first we will prepare the data, use some methods to generate more, and we will do segment prediction to get a segment and post process it, then use matching algorithm to trace each axon.</p>
<h4 id="Data-preparation"><a href="#Data-preparation" class="headerlink" title="Data  preparation"></a>Data  preparation</h4><ul>
<li><p>Extract axon segment (from Marco’s data)<br><img src="http://i2.tiimg.com/640680/9d013476a19d3dd8.png" alt="Markdown"><br><img src="http://i2.tiimg.com/640680/3c2d8c2fe9cab2e7.png" alt="Markdown"></p>
</li>
<li><p>Convert all segments to same color</p>
<ul>
<li>Training:    1200</li>
<li>Validation: 200<br><img src="http://i2.tiimg.com/640680/9e61c84bcda13810.png" alt="Markdown"></li>
</ul>
</li>
</ul>
<p>At first we use KK and marco’s data as training and validation sample. We convert the segment to same color as binary mask</p>
<h4 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h4><h3 id="Training"><a href="#Training" class="headerlink" title="Training:"></a>Training:</h3><ul>
<li><p>Simple augmentation: </p>
<ul>
<li>flip of x, y, (z); </li>
<li>90 degree rotation.<br><img src="http://i2.tiimg.com/640680/688954a7b2f85b05.png" alt="Markdown"></li>
</ul>
</li>
<li><p>Intensity augmentation.<br><img src="http://i2.tiimg.com/640680/d7f84ed10dc6977f.png" alt="Markdown"></p>
</li>
<li><p>Elastic augmentation<br><img src="http://i1.fuimg.com/640680/3e78d3e458b063b3.png" alt="Markdown"></p>
</li>
</ul>
<h5 id="Test"><a href="#Test" class="headerlink" title="Test:"></a>Test:</h5><p>Simple augmentation(16 combination)</p>
<p>Several augmentation methods are applied here to generate more training data, we have simple augmentation, intensity and elastic augmentation. For test part, we do all kinds of simple augmentation to get the average result<br>Although the augmentation May not have the strong biological meaning, but it is always useful to optimize the model better.</p>
<h4 id="Prediction-Model"><a href="#Prediction-Model" class="headerlink" title="Prediction Model"></a>Prediction Model</h4><p>We have discussed a lot about the prediction model, after a long time’s try, the 2D Dlinknet (adjustmen of U-net) finally works.</p>
<p>3D U-net with res block  (not very good)<br>2D D-LinkNet: encoder-decoder, res block, dilation.<br><img src="http://i2.tiimg.com/640680/70f0977b0a1fdfa5.png" alt="Markdown"></p>
<ul>
<li>Loss:<br>BCE+DICE loss(It seems remove DICE may have better result)<br><img src="http://i2.tiimg.com/640680/3b59f2842eaf0b18.png" alt="Markdown"></li>
</ul>
<p><img src="http://i2.tiimg.com/640680/713830f2720ff701.png" alt="Markdown"></p>
<p>Now we use a deep learning model to predict segmentation. I tried 3D U-net and 2D Link net to predict segment. It seems the 2D model is easier to train, for it has less parameters to tune and our data may have a big shift cross z-section. The model is similar to U-net, and the loss function we use is the combination of DICE loss and focal loss, which depict the overlap and difference of ground truth and prediction. The loss function decreases as training goes on.</p>
<p>which adopts encoderdecoder structure, dilated convolution and pretrained encoder, D-LinkNet architecture. Each blue rectangular block represents a multi-channel features map. Part A is the encoder of D-LinkNet. D-LinkNet uses ResNet34 as encoder. Part C is the decoder of D-LinkNet, it is set the same as LinkNet decoder. Original LinkNet only has Part A and Part C. D-LinkNet has an additional Part B which can enlarge the receptive field and as well as preserve the detailed spatial information. Each convolution layer is followed by a ReLU activation except the last convolution layer which use sigmoid activation.</p>
<p>reduces the relative loss for well-classified examples (pt &gt; .5), putting more focus on hard, misclassified examples. (we propose to reshape the loss function to down-weight easy examples and thus focus training on hard negatives. More formally, we propose to add a modulating factor (1 − pt) γ to the cross entropy loss, with tunable focusing parameter γ ≥ 0. We define the focal loss as)</p>
<h4 id="Prediction-Result"><a href="#Prediction-Result" class="headerlink" title="Prediction Result"></a>Prediction Result</h4><h3 id="Post-processing"><a href="#Post-processing" class="headerlink" title="Post processing:"></a>Post processing:</h3><ul>
<li>Bilateral filter</li>
<li>Erosion</li>
<li>Dilation</li>
</ul>
<p><img src="http://i2.tiimg.com/640680/3ccf827be716fafb.png" alt="Markdown"></p>
<p>I did some post processing work on prediction, using bilateral filter to remove some noise, Bilateral filter is better than gaussian filter. and use erosion and dilation to remove the potential merge of different connected region, since it is important to get sparse segment for next matching step, the dilation will make the segment smaller than the ground truth.<br>I evaluate it on validation set and the dice coefficient is acceptable since most of the region overlaps well.<br>Evaluation on Validation set</p>
<p><img src="http://i2.tiimg.com/640680/e24603eb94d53f89.png" alt="Markdown"></p>
<p><img src="http://i2.tiimg.com/640680/840d29a47250f731.png" alt="Markdown"></p>
<h1 id="NMJ-labeling-work"><a href="#NMJ-labeling-work" class="headerlink" title="NMJ labeling work"></a>NMJ labeling work</h1><p>Thanks to siyan and adi’s great work to align the image better, we can track and segment axons and terminals more easily. But the image quality isn’t good enough to apply automatic segmentation algorithm since it has many crack, noised and blur region. Now we have two magnitude more NMJs than six years ago,  our first ambition is to segment 13 NMJs to gain some<br>So after 1 month’s manual label we finally get the reconstruction result of 13 NMJs with 7 axons innervating them</p>
<p>It took us a month to reconstruct 13 NMJs, and there maybe 250 NMJs in total. So if we manually segment all NMJs, it will be at least two years effort. Which is too long to endure.<br><img src="http://i2.tiimg.com/640680/1b3e88542e0b6375.png" alt="Markdown"></p>
<p><img src="http://i2.tiimg.com/640680/659916b155960e05.png" alt="Markdown"></p>
<p><img src="http://i2.tiimg.com/640680/b064a79497b7cf37.png" alt="Markdown"></p>
<p><img src="http://i2.tiimg.com/640680/c5b7f35d11a3cc82.png" alt="Markdown"></p>
<p><img src="http://i2.tiimg.com/640680/749f5a6947ca871b.png" alt="Markdown"></p>
<p><img src="http://i2.tiimg.com/640680/0cc8c91faa68fc5c.png" alt="Markdown"></p>
<p><img src="http://i2.tiimg.com/640680/845ff7279a00f5f0.png" alt="Markdown"></p>
<p><img src="http://i2.tiimg.com/640680/7b9b3d2b77f9f277.png" alt="Markdown"></p>
<p>We would like to take advantage of what we have done here to do the work more quickily elsewhere. For example, since we really care about how much territory each axon has in each neural muscular junction. Does the incoming axon provides us any hints about that, that is to say, if we look at something like diameter of the incoming axon, can we get the information of  territory each axon has in NMJ. </p>
<p>For example, if the caliber of the axon and the contact area is propotional, we don’t need to reconstruct all of the junctions. That’s why we want to test if there is a correlation between diameter and contact area. So we need to quantify two things, one is axonal diameter, for each axon innervating each muscle fiber, to be more accurate, I extract 5 points to calculate diamter. and another one is the contact area.</p>
<p>Quantification of <strong>axonal diameter</strong></p>
<ul>
<li>5 points for each axon coming in each muscle fiber<br>Quantification of the <strong>contact area</strong></li>
<li>Contact area of each axon innervating each muscle fiber</li>
</ul>
<p>Firstly, for each axon, I calculate all the contact area it has with 13 muscle fibers and you can see different axons have different occupancy. The two largest have approximately 17 percent and the smallest one has less than nine percent</p>
<p><img src="http://i2.tiimg.com/640680/f06f7b963b5b305c.png" alt="Markdown"></p>
<p>Since  I have collected data for each axon coming into each muscle fiber, we can also have a look at them individually</p>
<p><img src="http://i2.tiimg.com/640680/59ecac8427b3a117.png" alt="Markdown"></p>
<p>This heatmap illustrate the diameter of each axon coming into each muscle fiber. And the white block means this axon has no contact with this muscle fiber, so I didn’t calculate it’s diameter</p>
<p><img src="http://i2.tiimg.com/640680/43d03374f4d2abd3.png" alt="Markdown"></p>
<p>This heatmap illustrate the contact area of each axon coming into each muscle fiber. And the white block means this axon has no contact with this muscle fiber, so I didn’t calculate it’s contact area</p>
<p>We can already have a sense that these two statistics may have some correlations by comparing these two heatmaps, by looking at these two plots together, I will gave you a scatter plot to see these correlation, although it is not perfect, but you will see a definite correlation between axonal diameter and contact area.</p>
<h3 id="Correlation-Test-Result"><a href="#Correlation-Test-Result" class="headerlink" title="Correlation Test Result"></a>Correlation Test Result</h3><p><img src="http://i2.tiimg.com/640680/3e61fe8864bdff61.png" alt="Markdown"></p>
<p>Pearson correlation coefficient:</p>
<script type="math/tex; mode=display">\rho_{X,Y} = \frac{cov(X,Y)}{\sigma_X \sigma_Y}</script><p>0.731 (p = 6.44×10^−12 )</p>
<p>Spearman correlation coefficient:  </p>
<script type="math/tex; mode=display">r_s = \rho_{rg_{X},rg_{Y}} = \frac{cov(g_{X},rg_{Y})}{\sigma_{rg_{X}} \sigma_{rg_{Y}} }</script><p>0.762 (p = 4.36*× 10^−12 )</p>
<p>Now we plot a scatter plot of all data points we collect, which are 7 axons coming into 13 muscle fibers, with some of them don’t have contact, there are more than seventy points. The x axis is axonal diameter and the y axis is contact area.  The units are microns and square microns. We have a regression line which fits the data well. Although it is not perfect. The shadow area is the confidence region of the regression.</p>
<p>By looking at this plot we have a sense that the axonal diameter and contact area should have a relatively linear correlation. We also use two metrics to quantify the correlation.<br>So one is pearson correlation coefficient, it is the covariance of two dataset divided by the individual standard deviation of two datasets. The range of this value is -1 to 1, the positive value means there is a positive correlation and the higher value means stronger correlation.<br>Now the pcc is 0.709 which indicates the correlation is good.<br>We also use another improved version of PCC to quantify the relative ranking correlation of the data. Which means we only focus on the relative value instead of the absolute value. It is Spearman correlation coefficient, and the result is a little higher. The good correlation means  if you get a bigger axonal diameter, it may have a relatively bigger contact area.</p>
<p>Apart from testing on all data together, we also test on each axon and each muscle fiber</p>
<h4 id="For-each-muscle"><a href="#For-each-muscle" class="headerlink" title="For each muscle"></a>For each muscle</h4><p><img src="http://i2.tiimg.com/640680/c8017110826d1fd5.png" alt="Markdown"></p>
<p><img src="http://i2.tiimg.com/640680/1ecfd3d4046d2dda.png" alt="Markdown"></p>
<p><img src="http://i2.tiimg.com/640680/a059b010a2dd1e39.png" alt="Markdown"></p>
<h4 id="For-each-axon"><a href="#For-each-axon" class="headerlink" title="For each axon"></a>For each axon</h4><p><img src="http://i2.tiimg.com/640680/ee09bea1b5c618d4.png" alt="Markdown"></p>
<p><img src="http://i2.tiimg.com/640680/5b0f66cfd920ac58.png" alt="Markdown"></p>

      
    </div>
    
    
    

    

    <div>
    
    <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-----The ---- end ----<i class="fa fa-paw"></i>--- Thanks --- for --- Reading----</div>
    
</div>

    
    </div>

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/blog/tags/project/" rel="tag"><i class="fa fa-tag"></i> project</a>
          
            <a href="/blog/tags/neural-science/" rel="tag"><i class="fa fa-tag"></i> neural science</a>
          
            <a href="/blog/tags/summer-intern/" rel="tag"><i class="fa fa-tag"></i> summer intern</a>
          
            <a href="/blog/tags/connectomics/" rel="tag"><i class="fa fa-tag"></i> connectomics</a>
          
            <a href="/blog/tags/computational-neural-science/" rel="tag"><i class="fa fa-tag"></i> computational neural science</a>
          
            <a href="/blog/tags/deep-learning/" rel="tag"><i class="fa fa-tag"></i> deep learning</a>
          
            <a href="/blog/tags/computer-vision/" rel="tag"><i class="fa fa-tag"></i> computer vision</a>
          
            <a href="/blog/tags/Jeff-Lichtman/" rel="tag"><i class="fa fa-tag"></i> Jeff Lichtman</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/blog/2018/06/20/39_wittgenstein_bio/" rel="next" title="Wittgenstein’s love and philosophy">
                <i class="fa fa-chevron-left"></i> Wittgenstein’s love and philosophy
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/blog/2018/07/14/98_summerintern_Synapse_Prediction/" rel="prev" title="Synapse Prediction Project">
                Synapse Prediction Project <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/blog/images/avatar.png"
                alt="James Chen" />
            
              <p class="site-author-name" itemprop="name">James Chen</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/blog/archives/">
              
                  <span class="site-state-item-count">90</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/blog/categories/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/blog/tags/index.html">
                  <span class="site-state-item-count">84</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/blog/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/james20141606" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:xp-chen14@mails.tsinghua.edu.cn" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#weekly-report"><span class="nav-number">1.</span> <span class="nav-text">weekly report</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#First"><span class="nav-number">1.1.</span> <span class="nav-text">First</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Second"><span class="nav-number">1.2.</span> <span class="nav-text">Second</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Third"><span class="nav-number">1.3.</span> <span class="nav-text">Third</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fourth"><span class="nav-number">1.4.</span> <span class="nav-text">Fourth</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#First-two-weeks"><span class="nav-number">2.</span> <span class="nav-text">First two weeks</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Future-work"><span class="nav-number">2.1.</span> <span class="nav-text">Future work</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week-3"><span class="nav-number">3.</span> <span class="nav-text">Week 3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#mask-and-seeding"><span class="nav-number">3.1.</span> <span class="nav-text">mask and seeding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#discussion-on-mask"><span class="nav-number">3.2.</span> <span class="nav-text">discussion on mask</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#My-thought-about-how-the-whole-project"><span class="nav-number">3.3.</span> <span class="nav-text">My thought about how the whole project</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Seeding-on-Mask3"><span class="nav-number">3.3.1.</span> <span class="nav-text">Seeding on Mask3</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Automatic-pipeline"><span class="nav-number">3.4.</span> <span class="nav-text">Automatic pipeline</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Predict-Membrane"><span class="nav-number">3.4.1.</span> <span class="nav-text">Predict Membrane</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Automatically-seeding"><span class="nav-number">3.4.2.</span> <span class="nav-text">Automatically seeding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#segmentation"><span class="nav-number">3.4.3.</span> <span class="nav-text">segmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Merge-masks"><span class="nav-number">3.4.4.</span> <span class="nav-text">Merge masks</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Algorithm"><span class="nav-number">3.5.</span> <span class="nav-text">Algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Predict-Membrane-1"><span class="nav-number">3.5.1.</span> <span class="nav-text">Predict Membrane</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Automatically-seeding-1"><span class="nav-number">3.5.2.</span> <span class="nav-text">Automatically seeding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Watershed"><span class="nav-number">3.5.3.</span> <span class="nav-text">Watershed</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Useful-resources"><span class="nav-number">3.5.4.</span> <span class="nav-text">Useful resources</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Work-on-membrane-prediction"><span class="nav-number">3.6.</span> <span class="nav-text">Work on membrane prediction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Prepare-ground-truth-training-set"><span class="nav-number">3.6.1.</span> <span class="nav-text">Prepare ground truth training set</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Train-membrane-prediction-model"><span class="nav-number">3.6.2.</span> <span class="nav-text">Train membrane prediction model</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Check-loss"><span class="nav-number">3.6.2.1.</span> <span class="nav-text">Check loss</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#real-time-monitoring-predicted-result"><span class="nav-number">3.6.2.2.</span> <span class="nav-text">real time monitoring predicted result</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week-5-amp-6"><span class="nav-number">4.</span> <span class="nav-text">Week 5 &amp; 6</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#predict-on-EM"><span class="nav-number">5.</span> <span class="nav-text">predict on EM</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#preprocess"><span class="nav-number">5.0.0.1.</span> <span class="nav-text">preprocess</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#try-to-predict-directly-on-masks-Not-the-membrane"><span class="nav-number">5.0.0.2.</span> <span class="nav-text">try to predict directly on masks. Not the membrane</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-31-retrain-on-deflicker-data"><span class="nav-number">5.0.1.</span> <span class="nav-text">7.31 retrain on deflicker data</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#NMJ-manually-labeling-work"><span class="nav-number">5.0.1.1.</span> <span class="nav-text">NMJ manually labeling work</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Add-more-data-for-automatic-pipeline"><span class="nav-number">5.0.1.2.</span> <span class="nav-text">Add more data for automatic pipeline</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#updated-pipeline"><span class="nav-number">5.0.2.</span> <span class="nav-text">updated pipeline</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#resolution-matters"><span class="nav-number">5.0.2.1.</span> <span class="nav-text">resolution matters!</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-11-new-try"><span class="nav-number">5.0.2.2.</span> <span class="nav-text">8.11 new try</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2D-D-Linknet"><span class="nav-number">5.0.2.3.</span> <span class="nav-text">2D D-Linknet</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Last-three-weeks"><span class="nav-number">6.</span> <span class="nav-text">Last three weeks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Automatic-Tracing-in-Bundle"><span class="nav-number">6.0.1.</span> <span class="nav-text">Automatic Tracing in Bundle</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Data-preparation"><span class="nav-number">6.0.1.1.</span> <span class="nav-text">Data  preparation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Data-Augmentation"><span class="nav-number">6.0.1.2.</span> <span class="nav-text">Data Augmentation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Training"><span class="nav-number">6.0.2.</span> <span class="nav-text">Training:</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Test"><span class="nav-number">6.0.2.0.1.</span> <span class="nav-text">Test:</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Prediction-Model"><span class="nav-number">6.0.2.1.</span> <span class="nav-text">Prediction Model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Prediction-Result"><span class="nav-number">6.0.2.2.</span> <span class="nav-text">Prediction Result</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Post-processing"><span class="nav-number">6.0.3.</span> <span class="nav-text">Post processing:</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#NMJ-labeling-work"><span class="nav-number">7.</span> <span class="nav-text">NMJ labeling work</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Correlation-Test-Result"><span class="nav-number">7.0.1.</span> <span class="nav-text">Correlation Test Result</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#For-each-muscle"><span class="nav-number">7.0.1.1.</span> <span class="nav-text">For each muscle</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#For-each-axon"><span class="nav-number">7.0.1.2.</span> <span class="nav-text">For each axon</span></a></li></ol></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">James Chen</span>

  
</div>



  <span class="post-meta-divider">|</span>




<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共161.4k字</span>
</div>

        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/blog/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/blog/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/blog/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/blog/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/blog/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/blog/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  



  


  <script type="text/javascript" src="/blog/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/blog/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/blog/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/blog/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/blog/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("klOjl0RBA8qP5IKgIXkOszBr-gzGzoHsz", "rCaN5wX4mjiRkRMzP95g7XHz");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  


  

  

  
</body>
</html>
